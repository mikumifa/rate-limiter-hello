## 云原生大作业说明文档
### 组员信息及分工
|姓名|学号|邮箱|分工|
|-|-|-|-|
|赵政杰|211250109|3239168744@qq.com|文档书写，Prometheus+grafana配置监控及压力测试|
|徐子豪|211250110|||
|谢其卓|211870187|1055069518@qq.com|Spring Boot 应用部分功能, DevOps部分功能|
|本组为 nju23 组||||
|本组使用 github 仓库进行协同开发，通过 QQ 合作交流||||
|github仓库地址：https://github.com/mikumifa/rate-limiter-hello.git||||
### 目录
[toc]
### 作业要求
开发一个 Spring Boot 应用，并使用云原生功能
具体要求：https://doc.weixin.qq.com/doc/w3_AK4AcQYdAN0amWA7Kk4RX2ulwKZbs?scode=ABoAuwfGAAkWJM0xI3AK4AcQYdAN0

### 项目实现
#### 1.基本功能

##### 1.1 实现接口 

![image-20230814211426438](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814211426438.png)

实现hello接口，

![image-20230814211529008](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814211529008.png)

请求会打印hello， 同时显示当前这一秒有多少次请求次数

##### 1.2 实现限流功能

自己实现了简单的令牌桶， 使用了数据库的定时事务

```sql
DROP TABLE IF EXISTS `pod_rate_limit`;
CREATE TABLE `pod_rate_limit`  (
  `id` int NOT NULL AUTO_INCREMENT,
  `token` int NULL DEFAULT NULL,
  `last_updated` timestamp NULL DEFAULT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 2 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ROW_FORMAT = Dynamic;
```

定时事务

```sql
-- 创建一个事件调度器
DELIMITER //
CREATE EVENT IF NOT EXISTS `update_token_event`
ON SCHEDULE EVERY 1 SECOND
DO
BEGIN
    -- 更新token字段为100并更新last_updated字段为当前时间
    UPDATE `pod_rate_limit`
    SET `token` = 100,
        `last_updated` = CURRENT_TIMESTAMP;
END;
//
DELIMITER ;

```

![image-20230814212104688](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814212104688.png)

每访问一次数据， 数据库都会减去1， 同时可以查询当前数据库的值， 从而可以判断当前是否有令牌， 因为1秒钟内， 令牌数目是一定的， 所以可以实现限流， 在hello接口里面

##### 1.3实现接口访问指标（QPS），并暴露给 Prometheus

1. **Counter定义和注册**：使用了Prometheus的Java客户端库来创建一个Counter（计数器）指标，该计数器用于记录接口请求的总次数。通过以下代码段，定义了一个名为"hello_requests_total"的计数器，并为其提供了一个描述。然后，使用`.register()`方法将其注册到Prometheus的默认注册表中。

   ```java
   private final Counter requestsCounter = Counter.build()
           .name("hello_requests_total")
           .help("Total number of hello requests")
           .register();
   ```

2. **接口逻辑**：在`/hello`端点的GET请求处理方法中，首先进行了一些业务逻辑，然后根据情况更新计数器。每当该端点被访问时，使用`requestsCounter.inc()`方法来增加计数器的值，表示有新的请求。这是实现QPS（每秒请求数）的关键步骤，因为在每次请求时都增加了计数器的值。

   ```java
   // Increment the counter
   requestsCounter.inc();
   ```

3. **暴露给 Prometheus**：访问`http://localhost:8080/actuator/prometheus`（假设你的应用程序在本地运行并监听8080端口）。这将返回一个文本响应，其中包含了暴露的指标数据，包括你在代码中定义的计数器。

   配置如下

   ![image-20230814214422582](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814214422582.png)

##### 1.3 **统一限流** （bouns）

每访问一次数据， 数据库都会减去1， 同时可以查询当前数据库的值， 从而可以判断当前是否有令牌， 因为1秒钟内， 令牌数目是一定的， 所以可以实现限流， 在hello接口里面。

由于所有的pod使用同一个数据库， 所有pod使用公用一个令牌桶， 从而实现统一的限流。

![image-20230814214601383](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814214601383.png)

效果截图



使用ab发送1000次请求![image-20230814225037243](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814225037243.png)

观察每一个pods的日志， ![image-20230814225000893](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814225000893.png)

观察到了 请求过多之后打印Too many requests，和token重置的现象， 并且由于所有的pod公用数据库，pods之间实现统一的限流![image-20230814225206890](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814225206890.png)

下面是一次请求过程中3个pod的日志

![image-20230814225539418](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814225539418.png)

![image-20230814225609474](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814225609474.png)

![image-20230814225631941](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814225631941.png)

#### 2.DevOps功能

##### 2.1 Dockerfile 用于构建镜像

说明以注释的方式， 其中换源主要是为了内部测试使用。

![image-20230814214727884](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814214727884.png)

##### 2.2 Kubernetes 编排文件

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hello
  name: hello
  namespace: nju23
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hello
  template:
    metadata:
      annotations:
        prometheus.io/path: /actuator/prometheus
        prometheus.io/port: "8080"
        prometheus.io/scheme: http
        prometheus.io/scrape: "true"
      labels:
        app: hello
    spec:
      containers:
        - name: hello
          image: harbor.edu.cn/nju23/hello:{VERSION}
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080

---
apiVersion: v1
kind: Service
metadata:
  name: hello
  namespace: nju23
  labels:
    app: hello
spec:

  selector:
    app: hello
  ports:
    - name: tcp
      nodePort: 31047
      protocol: TCP
      port: 8080
      targetPort: 8080
  type: NodePort
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: hello
  name: hello
  namespace: nju23
spec:
  endpoints:
    - interval: 30s
      port: tcp
      path: /actuator/prometheus
      scheme: 'http'
  selector:
    matchLabels:
      app: hello
  namespaceSelector:
    matchNames:
      - nju23

```

安装官方文档要求正常的设置，用于在 Kubernetes 集群上创建 Deployment 和 Service， 还有ServiceMonitor

##### 2.3 持续集成流水线

实现**代码构建**/**单元测试**/**镜像构建**功能

其中代码构建阶段， 从git中拉取代码， 然后使用maven构建， maven构建过程中会自动的进行单元测试（下面是我们的单元测试，用于压力测试）

![image-20230814215339313](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814215339313.png)

最后会使用

```
           sh 'docker build -f Dockerfile -t hello:${BUILD_ID} . '
            sh 'docker tag  hello:${BUILD_ID}  harbor.edu.cn/nju23/hello:${BUILD_ID}'
```

来构建镜像， 其中使用${BUILD_ID} 来实现每一次构建的镜像都是最新的。



```
stages {
        stage('Clone Code') {
            agent {
                label 'master'
            }
            steps {
                echo "1.Git Clone Code"

            script {
            def gitUrl = "https://gitclone.com/github.com/mikumifa/rate-limiter-hello.git"
            def gitBranch = "main" // 使用main分支
            
            checkout([
                $class: 'GitSCM',
                branches: [[name: "refs/remotes/origin/${gitBranch}"]],
                doGenerateSubmoduleConfigurations: false,
                extensions: [[$class: 'CloneOption', noTags: true, reference: '', shallow: true]],
                userRemoteConfigs: [[url: gitUrl]]
            ])
        }
            }
        }
        stage('Maven Build') {
            agent {
                docker {
                    image 'maven:latest'
                    args '-v /root/.m2:/root/.m2'
                }
            }
            steps {
                echo "2.Maven Build Stage"
                sh 'mvn -B clean package -Dmaven.test.skip=false'
            }
        }
        stage('Test'){
            agent{
                docker {
                    image 'maven:latest'
                    args '-v /root/.m2:/root/.m2'
                }
            }
            steps{
                echo "2.1.Start Test"
                sh 'mvn test'
            }
        }
        stage('Image Build') {
            agent {
                label 'master'
            }
            steps {
            echo "3.Image Build Stage"
            sh 'docker build -f Dockerfile -t hello:${BUILD_ID} . '
            sh 'docker tag  hello:${BUILD_ID}  harbor.edu.cn/nju23/hello:${BUILD_ID}'
            }
        }
        stage('Push') {
            agent {
                label 'master'
            }
            steps {
            echo "4.Push Docker Image Stage"
            sh "docker login --username=nju23 harbor.edu.cn -p nju232023"
            sh "docker push harbor.edu.cn/nju23/hello:${BUILD_ID}"
            }
        }
    }
```

##### 2.4 持续部署流水线

部署流水线， ，实现部署到 Kubernetes 集群的功能，该流水线的触发条件为持续集成流水线执行成功， 不过持续集成流水线和持续部署流水线合二为一了。

```

node('slave') {
    container('jnlp-kubectl') {
        
        stage('Clone YAML') {
        echo "5. Git Clone YAML To Slave"
        script {
            def gitUrl = "https://gitclone.com/github.com/mikumifa/rate-limiter-hello.git"
            def gitBranch = "main" // 使用main分支
            
            checkout([
                $class: 'GitSCM',
                branches: [[name: "refs/remotes/origin/${gitBranch}"]],
                doGenerateSubmoduleConfigurations: false,
                extensions: [[$class: 'CloneOption', noTags: true, reference: '', shallow: true]],
                userRemoteConfigs: [[url: gitUrl]]
            ])
        }

        }
        
        stage('YAML') {
        echo "6. Change YAML File Stage"
        sh 'sed -i "s#{VERSION}#${BUILD_ID}#g" ./jenkins/scripts/deployment.yaml'

        }
    
        stage('Deploy') {
        echo "7. Deploy To K8s Stage"
        sh 'kubectl apply -f ./jenkins/scripts/deployment.yaml -n nju23'
        }
    }
```

下图是后续验证流水线成功的截图（部署失败的，大部分是因为git clone不成功）

![image-20230814215744509](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814215744509.png)

![image-20230814215714650](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814215714650.png)

**2.5 代码提交到仓库自动触发流水线**

使用了github 的git action来触发流水线

1. **在 GitHub 仓库中设置 Secrets**：

   在您的 GitHub 仓库页面上，点击 "Settings" > "Secrets" > "New repository secret"，然后添加名为 `STUDENT_ID` 和 `PASSWORD` 的两个 secret，分别学号和密码。

2. **创建工作流程文件**：

   在代码仓库中，创建一个 `.github/workflows` 目录，然后在该目录下创建一个 `.yml` 格式的工作流程文件，例如 `trigger-pipeline.yml`。

```
name: Trigger Jenkins Pipeline

on:
  push:
    branches:
      - main

jobs:
  trigger_pipeline:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v2

      - name: Set up curl
        run: sudo apt-get install -y curl

      - name: Trigger Pipeline
        env:
          STUDENT_ID: ${{ secrets.STUDENT_ID }}
          PASSWORD: ${{ secrets.PASSWORD }}
        run: |
          curl "http://p.nju.edu.cn/portal_io/login?username=${STUDENT_ID}&password=${PASSWORD}"
          curl -X POST http://172.29.4.36:8080/job/023-pipeline/build

```

 当git 提交时候， 会触发git action， git action通过脚本触发Jenkins

![image-20230814223030633](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814223030633.png)

![image-20230814223042502](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814223042502.png)

#### 3.扩容场景
##### 3.1Prometheus与Grafana的可视化监控
编写`prometheus.yml`文件，通过该 Java 项目提供的/actuator/prometheus接口，以供 Prometheus 采集监控指标
目标是本机ip的8080端口（项目运行在容器中时，通过8080端口映射）
![](https://box.nju.edu.cn/f/9f1c872c2b014ed7b7ba/?dl=1)
编写 docker 配置文件`docker-compose.yml`，它用来在 docker 上安装和配置 Grafana
其中 Grafana 使用3030端口，Prometheus 使用9090端口
![](https://box.nju.edu.cn/f/d9fed3361b98412a9666/?dl=1)
运行`docker-compose.yml`配置文件，可以看到 docker 中有两个正在运行的容器，分别为 Grafana 和 Prometheus，它们在同一网络下
![](https://box.nju.edu.cn/f/041dc413a3894f9c9daf/?dl=1)
运行已经构建好的`hello-rate-limiter`镜像（映射到本机的8080端口）
![](https://box.nju.edu.cn/f/82a8fbd25d654e31842c/?dl=1)
查看 Prometheus 的端口，可以看到 target 中 spring-boot 的监测接口是 up 状态
![](https://box.nju.edu.cn/f/d1437dde9d2848dda635/?dl=1)
查看 Grafana 的端口，用户名和初始密码均为`admin`
![](https://box.nju.edu.cn/f/353fd7aad60a41a6a064/?dl=1)

##### 3.2Grafana 定制应用监控大屏
首先设置数据源，进入 Grafana 后，在左侧 Data Source 即数据源选项，点 Add Data Source 即添加数据源，选择 Prometheus
![](https://box.nju.edu.cn/f/da50418b0f4c4a40a338/?dl=1)
![](https://box.nju.edu.cn/f/479fd4280f8a41f59616/?dl=1)
之后设置数据源 URL，填入`http://prometheus-1:9090`（prometheus-1为 Prometheus 容器的名字）
![](https://box.nju.edu.cn/f/5c4e95d044ae43989ff7/?dl=1)
之后点击`save and test`，看到`Data source is working`，表明 Grafana 已经联系到 Prometheus
![](https://box.nju.edu.cn/f/cacb851c53e84b00b84c/?dl=1)
使用图形化界面配置仪表盘，分别检测该项目的CPU使用率（system_cpu_usage）、内存使用率（jvm_memory_used_bytes）和每10s的平均http请求数（rate(http_server_requests_seconds_count[10s])
![](https://box.nju.edu.cn/f/4f7799255caf4e28ae2a/?dl=1)
![](https://box.nju.edu.cn/f/d90a11a9a95d41928b0f/?dl=1)
![](https://box.nju.edu.cn/f/53337052ba6a4e4c937c/?dl=1)
最后的效果如下
![](https://box.nju.edu.cn/f/1982d1f9f0d744e0a1a4/?dl=1)
##### 3.3通过使用 Jmeter 工具对接口进压测，在 Grafana 中观察监控数据
在 Jmeter 中使用300线程，循环5次对接口`http://localhost:8080/hello`进行压力测试
![](https://box.nju.edu.cn/f/fa52f8aa83ee452b8cbb/?dl=1)
压测前的状态（最近五分钟）
![](https://box.nju.edu.cn/f/41431ac632b447689fd1/?dl=1)
压测后的状态（最近五分钟）
![](https://box.nju.edu.cn/f/ac566e564d2a4c34bc96/?dl=1)
可以看到，压测过程中CPU使用率和http请求数都增大一段时间
在 Jmeter 中也出现了访问次数过多的返回429的情况
![](https://box.nju.edu.cn/f/dde5884866554d0fab7e/?dl=1)

**3.5 Auto Scale（bonus）**

使用 Kubernetes HPA 模块根据 CPU 负载做服务的 Auto Scale

创建一个Horizontal Pod Autoscaler (HPA) 的配置文件，例如 `hpa.yaml`，示例如下：

![image-20230814223531520](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814223531520.png)

- `scaleTargetRef`中的 `name` 字段应该与Deployment的名称匹配。
- `minReplicas` 指定了最小的Pod副本数。
- `maxReplicas` 指定了最大的Pod副本数。
- `metrics` 部分指定了用于自动扩缩容的指标，这里使用了CPU资源利用率作为指标。

当应用开始运行后，HPA会定期检查CPU利用率，并根据配置的目标值进行扩缩容。如果CPU利用率超过了目标值，HPA会增加Pod副本数，以应对负载增加；反之，如果CPU利用率低于目标值，HPA会减少Pod副本数，以节约资源。

下面是截图

配置的截图

![image-20230814223633620](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814223633620.png)

查看hpa

![image-20230814223647272](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814223647272.png)

开始时候的pod数目![image-20230814223707155](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814223707155.png)

使用ab去发送100次请求

![image-20230814224020908](https://blog-1314638240.cos.ap-nanjing.myqcloud.com/image/image-20230814224020908.png)

通过观察验证HPA是否根据CPU负载自动进行了扩缩容操作